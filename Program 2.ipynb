{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51481fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    return 1.0 - np.sum(probabilities**2)\n",
    "\n",
    "def best_split(X, y):\n",
    "    num_samples, num_features = X.shape\n",
    "    if num_samples <= 1:\n",
    "        return None, None\n",
    "\n",
    "    current_impurity = gini_impurity(y)\n",
    "\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_impurity_reduction = 0\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            left_mask = X[:, feature_index] <= threshold\n",
    "            right_mask = ~left_mask\n",
    "\n",
    "            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            left_impurity = gini_impurity(y[left_mask])\n",
    "            right_impurity = gini_impurity(y[right_mask])\n",
    "            impurity_reduction = current_impurity - (\n",
    "                len(y[left_mask]) / len(y) * left_impurity +\n",
    "                len(y[right_mask]) / len(y) * right_impurity\n",
    "            )\n",
    "\n",
    "            if impurity_reduction > best_impurity_reduction:\n",
    "                best_feature = feature_index\n",
    "                best_threshold = threshold\n",
    "                best_impurity_reduction = impurity_reduction\n",
    "\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e71ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(X, y, depth, max_depth):\n",
    "    num_samples, _ = X.shape\n",
    "    unique_classes = np.unique(y)\n",
    "\n",
    "    if len(unique_classes) == 1 or (max_depth is not None and depth == max_depth):\n",
    "        return {'class': unique_classes[0]}\n",
    "\n",
    "    best_feature, best_threshold = best_split(X, y)\n",
    "\n",
    "    if best_feature is None:\n",
    "        return {'class': np.bincount(y).argmax()}\n",
    "\n",
    "    left_mask = X[:, best_feature] <= best_threshold\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    left_subtree = grow_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n",
    "    right_subtree = grow_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n",
    "\n",
    "    return {'feature_index': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree}\n",
    "\n",
    "def predict_sample(x, node):\n",
    "    if 'class' in node:\n",
    "        return node['class']\n",
    "    if x[node['feature_index']] <= node['threshold']:\n",
    "        return predict_sample(x, node['left'])\n",
    "    else:\n",
    "        return predict_sample(x, node['right'])\n",
    "\n",
    "def predict(X, tree):\n",
    "    return np.array([predict_sample(x, tree) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b55d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ce800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "df = pd.read_csv(\"/Users/mukulhooda/Desktop/College/3rd Year/Machine Learning-1/Lab File/Programs/Iris - Iris.csv\")\n",
    "\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})\n",
    "df.head()\n",
    "\n",
    "\n",
    "def train_test_split(df, test_size):\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    return train_df, test_df\n",
    "\n",
    "def check_purity(data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def classify_data(data):\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    return classification\n",
    "\n",
    "\n",
    "def get_potential_splits(data):\n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    for column_index in range(n_columns - 1):\n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "    split_column_values = data[:, split_column]\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values > split_value]\n",
    "    return data_below, data_above\n",
    "\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    label_column = data[:, -1]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "    overall_entropy = (p_data_below * calculate_entropy(data_below)\n",
    "                       + p_data_above * calculate_entropy(data_above))\n",
    "    return overall_entropy\n",
    "\n",
    "\n",
    "def determine_best_split(data, potential_splits):\n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "\n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "sub_tree = {\"question\": [\"yes_answer\",\n",
    "                         \"no_answer\"]}\n",
    "example_tree = {\"petal_width <= 0.8\": [\"Iris-setosa\",\n",
    "                                       {\"petal_width <= 1.65\": [{\"petal_length <= 4.9\": [\"Iris-versicolor\",\n",
    "                                                                                         \"Iris-virginica\"]},\n",
    "                                                                \"Iris-virginica\"]}]}\n",
    "\n",
    "\n",
    "def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5):\n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df\n",
    "        # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        return classification\n",
    "    # recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "        # helper functions\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "\n",
    "        # instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        question = \"{} <= {}\".format(feature_name, split_value)\n",
    "        sub_tree = {question: []}\n",
    "\n",
    "        # find answers (recursion)\n",
    "        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n",
    "        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n",
    "\n",
    "        # If the answers are the same, then there is no point in asking the question.\n",
    "        # This could happen when the data is classified even though it is not pure\n",
    "        # yet (min_samples or max_depth base cases).\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        return sub_tree\n",
    "\n",
    "\n",
    "def classify_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split()\n",
    "\n",
    "    # ask question\n",
    "    if example[feature_name] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "\n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_example(example, residual_tree)\n",
    "\n",
    "\n",
    "def calculate_accuracy(df, tree):\n",
    "    df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df[\"label\"]\n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=20)\n",
    "tree = decision_tree_algorithm(train_df, max_depth=3)\n",
    "accuracy = calculate_accuracy(test_df, tree)\n",
    "print()\n",
    "pprint(tree)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for label in label_counts:\n",
    "        probability = label_counts[label] / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, labels, feature_index):\n",
    "    total_entropy = entropy(labels)\n",
    "    \n",
    "    feature_values = data[:, feature_index]\n",
    "    unique_values = np.unique(feature_values)\n",
    "    feature_entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for value in unique_values:\n",
    "        subset_indices = np.where(feature_values == value)[0]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subset_entropy = entropy(subset_labels)\n",
    "        weight = len(subset_indices) / total_samples\n",
    "        feature_entropy += weight * subset_entropy\n",
    "    \n",
    "    information_gain = total_entropy - feature_entropy\n",
    "    return information_gain\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    num_features = data.shape[1]\n",
    "    best_feature_index = None\n",
    "    best_information_gain = -1\n",
    "    for feature_index in range(num_features):\n",
    "        ig = information_gain(data, labels, feature_index)\n",
    "        if ig > best_information_gain:\n",
    "            best_information_gain = ig\n",
    "            best_feature_index = feature_index\n",
    "    return best_feature_index\n",
    "\n",
    "def build_tree(data, labels, feature_indices):\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # If there is only one class in labels, return a leaf node with that class\n",
    "    if len(unique_labels) == 1:\n",
    "        return unique_labels[0]\n",
    "\n",
    "    # If there are no more features to split on, return the most common label\n",
    "    if len(feature_indices) == 0:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "    # Find the best feature to split on\n",
    "    best_feature_index = find_best_split(data, labels)\n",
    "\n",
    "    # Create a new tree node with the best feature\n",
    "    tree_node = {'feature_index': best_feature_index, 'children': {}}\n",
    "    feature_indices.remove(best_feature_index)\n",
    "\n",
    "    # Recursively build subtrees\n",
    "    feature_values = np.unique(data[:, best_feature_index])\n",
    "    for value in feature_values:\n",
    "        subset_indices = np.where(data[:, best_feature_index] == value)[0]\n",
    "        subset_data = data[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subtree = build_tree(subset_data, subset_labels, feature_indices[:])\n",
    "        tree_node['children'][value] = subtree\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "def fit(data, labels):\n",
    "    num_features = data.shape[1]\n",
    "    feature_indices = list(range(num_features))\n",
    "    return build_tree(data, labels, feature_indices)\n",
    "\n",
    "def predict(tree, sample):\n",
    "    def traverse(node, sample):\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature_index']\n",
    "            value = sample[feature_index]\n",
    "            if value in node['children']:\n",
    "                return traverse(node['children'][value], sample)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return traverse(tree, sample)\n",
    "\n",
    "# Example usage:\n",
    "# data = np.array([\n",
    "#     [1, 1],\n",
    "#     [1, 0],\n",
    "#     [0, 1],\n",
    "#     [0, 0],\n",
    "#     [1, 1],\n",
    "#     [1, 0],\n",
    "#     [0, 1],\n",
    "#     [0, 0]\n",
    "# ])\n",
    "\n",
    "# labels = np.array([1, 1, 1, 0, 1, 1, 0, 0])\n",
    "\n",
    "data=pd.read_csv('/Users/mukulhooda/Documents/Workbooks /Assignment 2.csv')\n",
    "# tree = fit(data, labels)\n",
    "\n",
    "# # Predictions for new samples\n",
    "# new_samples = np.array([\n",
    "#     [1, 0],\n",
    "#     [0, 1]\n",
    "# ])\n",
    "# for sample in new_samples:\n",
    "#     prediction = predict(tree, sample)\n",
    "#     print(f\"Prediction for {sample}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfe2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for label in label_counts:\n",
    "        probability = label_counts[label] / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, labels, feature_index):\n",
    "    total_entropy = entropy(labels)\n",
    "    \n",
    "    feature_values = data[:, feature_index]\n",
    "    unique_values = np.unique(feature_values)\n",
    "    feature_entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for value in unique_values:\n",
    "        subset_indices = np.where(feature_values == value)[0]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subset_entropy = entropy(subset_labels)\n",
    "        weight = len(subset_indices) / total_samples\n",
    "        feature_entropy += weight * subset_entropy\n",
    "    \n",
    "    information_gain = total_entropy - feature_entropy\n",
    "    return information_gain\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    num_features = data.shape[1] - 1\n",
    "    best_feature_index = None\n",
    "    best_information_gain = -1\n",
    "    for feature_index in range(num_features):\n",
    "        ig = information_gain(data, labels, feature_index)\n",
    "        if ig > best_information_gain:\n",
    "            best_information_gain = ig\n",
    "            best_feature_index = feature_index\n",
    "    return best_feature_index\n",
    "\n",
    "def build_tree(data, labels, feature_indices):\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # If there is only one class in labels, return a leaf node with that class\n",
    "    if len(unique_labels) == 1:\n",
    "        return unique_labels[0]\n",
    "\n",
    "    # If there are no more features to split on, return the most common label\n",
    "    if len(feature_indices) == 0:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "    # Find the best feature to split on\n",
    "    best_feature_index = find_best_split(data, labels)\n",
    "\n",
    "    # Create a new tree node with the best feature\n",
    "    tree_node = {'feature_index': best_feature_index, 'children': {}}\n",
    "    feature_indices.remove(best_feature_index)\n",
    "\n",
    "    # Recursively build subtrees\n",
    "    feature_values = np.unique(data[:, best_feature_index])\n",
    "    for value in feature_values:\n",
    "        subset_indices = np.where(data[:, best_feature_index] == value)[0]\n",
    "        subset_data = data[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subtree = build_tree(subset_data, subset_labels, feature_indices[:])\n",
    "        tree_node['children'][value] = subtree\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "def fit(data, labels):\n",
    "    num_features = data.shape[1] - 1\n",
    "    feature_indices = list(range(num_features))\n",
    "    return build_tree(data, labels, feature_indices)\n",
    "\n",
    "def predict(tree, sample):\n",
    "    def traverse(node, sample):\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature_index']\n",
    "            value = sample[feature_index]\n",
    "            if value in node['children']:\n",
    "                return traverse(node['children'][value], sample)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return traverse(tree, sample)\n",
    "\n",
    "def load_data_from_csv(csv_file):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append([int(x) for x in row[:-1]])\n",
    "            labels.append(int(row[-1]))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load data from CSV file\n",
    "csv_file = '/Users/mukulhooda/Documents/Workbooks /Assignment 2.csv'  # Provide the path to your CSV file here\n",
    "data, labels = load_data_from_csv(csv_file)\n",
    "\n",
    "# Fit the decision tree\n",
    "tree = fit(X, Y)\n",
    "\n",
    "# Predictions for new samples\n",
    "new_samples = np.array([\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 0]\n",
    "])\n",
    "for sample in new_samples:\n",
    "    prediction = predict(tree, sample)\n",
    "    print(f\"Prediction for {sample}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/Users/mukulhooda/Documents/Workbooks /Assignment 2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Select X and Y Columns\n",
    "# Replace 'feature_cols' and 'target_col' with your actual column names\n",
    "feature_cols = ['Day','Outlook','Humidity','Wind','Play Tennis']  # List of column names for features (X)\n",
    "target_col = 'Play Tennis'  # Name of the column for the target variable (Y)\n",
    "\n",
    "# Step 4: Extract X and Y\n",
    "X = data[feature_cols].values  # Extract features as numpy array\n",
    "Y = data[target_col].values  # Extract target variable as numpy array\n",
    "\n",
    "# You can now use X and Y for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6995f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for label in label_counts:\n",
    "        probability = label_counts[label] / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, labels, feature_index):\n",
    "    total_entropy = entropy(labels)\n",
    "    \n",
    "    feature_values = data[:, feature_index]\n",
    "    unique_values = np.unique(feature_values)\n",
    "    feature_entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for value in unique_values:\n",
    "        subset_indices = np.where(feature_values == value)[0]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subset_entropy = entropy(subset_labels)\n",
    "        weight = len(subset_indices) / total_samples\n",
    "        feature_entropy += weight * subset_entropy\n",
    "    \n",
    "    information_gain = total_entropy - feature_entropy\n",
    "    return information_gain\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    num_features = data.shape[1] - 1\n",
    "    best_feature_index = None\n",
    "    best_information_gain = -1\n",
    "    for feature_index in range(num_features):\n",
    "        ig = information_gain(data, labels, feature_index)\n",
    "        if ig > best_information_gain:\n",
    "            best_information_gain = ig\n",
    "            best_feature_index = feature_index\n",
    "    return best_feature_index\n",
    "\n",
    "def build_tree(data, labels, feature_indices):\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # If there is only one class in labels, return a leaf node with that class\n",
    "    if len(unique_labels) == 1:\n",
    "        return unique_labels[0]\n",
    "\n",
    "    # If there are no more features to split on, return the most common label\n",
    "    if len(feature_indices) == 0:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "    # Find the best feature to split on\n",
    "    best_feature_index = find_best_split(data, labels)\n",
    "\n",
    "    # Create a new tree node with the best feature\n",
    "    tree_node = {'feature_index': best_feature_index, 'children': {}}\n",
    "    feature_indices.remove(best_feature_index)\n",
    "\n",
    "    # Recursively build subtrees\n",
    "    feature_values = np.unique(data[:, best_feature_index])\n",
    "    for value in feature_values:\n",
    "        subset_indices = np.where(data[:, best_feature_index] == value)[0]\n",
    "        subset_data = data[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subtree = build_tree(subset_data, subset_labels, feature_indices[:])\n",
    "        tree_node['children'][value] = subtree\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "def fit(data, labels):\n",
    "    num_features = data.shape[1] - 1\n",
    "    feature_indices = list(range(num_features))\n",
    "    return build_tree(data, labels, feature_indices)\n",
    "\n",
    "def predict(tree, sample):\n",
    "    def traverse(node, sample):\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature_index']\n",
    "            value = sample[feature_index]\n",
    "            if value in node['children']:\n",
    "                return traverse(node['children'][value], sample)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return traverse(tree, sample)\n",
    "\n",
    "def load_data_from_csv(csv_file):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append([int(x) for x in row[:-1]])\n",
    "            labels.append(int(row[-1]))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load data from CSV file\n",
    "# csv_file = 'data.csv'  # Provide the path to your CSV file here\n",
    "# data, labels = load_data_from_csv(csv_file)\n",
    "\n",
    "# Fit the decision tree\n",
    "tree = fit(X, Y)\n",
    "\n",
    "# Predictions for new samples\n",
    "new_samples = np.array([\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 1, 0, 1, 0]\n",
    "])\n",
    "for sample in new_samples:\n",
    "    prediction = predict(tree, sample)\n",
    "    print(f\"Prediction for {sample}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d355796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for label in label_counts:\n",
    "        probability = label_counts[label] / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, labels, feature_index):\n",
    "    total_entropy = entropy(labels)\n",
    "    \n",
    "    feature_values = data[:, feature_index]\n",
    "    unique_values = np.unique(feature_values)\n",
    "    feature_entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for value in unique_values:\n",
    "        subset_indices = np.where(feature_values == value)[0]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subset_entropy = entropy(subset_labels)\n",
    "        weight = len(subset_indices) / total_samples\n",
    "        feature_entropy += weight * subset_entropy\n",
    "    \n",
    "    information_gain = total_entropy - feature_entropy\n",
    "    return information_gain, feature_entropy\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    num_features = data.shape[1] - 1  # Exclude the last column which is the dependent variable\n",
    "    best_feature_index = None\n",
    "    best_information_gain = -1\n",
    "    for feature_index in range(num_features):\n",
    "        ig, impurity = information_gain(data, labels, feature_index)\n",
    "        if ig > best_information_gain:\n",
    "            best_information_gain = ig\n",
    "            best_feature_index = feature_index\n",
    "    return best_feature_index\n",
    "\n",
    "def build_tree(data, labels, feature_indices):\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # If there is only one class in labels, return a leaf node with that class\n",
    "    if len(unique_labels) == 1:\n",
    "        return unique_labels[0]\n",
    "\n",
    "    # If there are no more features to split on, return the most common label\n",
    "    if len(feature_indices) == 0:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "    # Find the best feature to split on\n",
    "    best_feature_index = find_best_split(data, labels)\n",
    "\n",
    "    # Create a new tree node with the best feature\n",
    "    tree_node = {'feature_index': best_feature_index, 'children': {}}\n",
    "    feature_indices.remove(best_feature_index)\n",
    "\n",
    "    # Recursively build subtrees\n",
    "    feature_values = np.unique(data[:, best_feature_index])\n",
    "    for value in feature_values:\n",
    "        subset_indices = np.where(data[:, best_feature_index] == value)[0]\n",
    "        subset_data = data[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        subtree = build_tree(subset_data, subset_labels, feature_indices[:])\n",
    "        tree_node['children'][value] = subtree\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "def fit(data, labels):\n",
    "    num_features = data.shape[1] - 1  # Exclude the last column which is the dependent variable\n",
    "    feature_indices = list(range(num_features))\n",
    "    return build_tree(data, labels, feature_indices)\n",
    "\n",
    "def predict(tree, sample):\n",
    "    def traverse(node, sample):\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature_index']\n",
    "            value = sample[feature_index]\n",
    "            if value in node['children']:\n",
    "                return traverse(node['children'][value], sample)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return traverse(tree, sample)\n",
    "\n",
    "def load_data_from_csv(csv_file):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append([int(x) for x in row[:-1]])\n",
    "            labels.append(int(row[-1]))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def print_information_gain(data, labels):\n",
    "    num_features = data.shape[1] - 1  # Exclude the last column which is the dependent variable\n",
    "    for feature_index in range(num_features):\n",
    "        ig, impurity = information_gain(data, labels, feature_index)\n",
    "        print(f\"Information Gain for feature {feature_index}: {ig:.4f}\")\n",
    "        print(f\"Mini Impurity for feature {feature_index}: {impurity:.4f}\\n\")\n",
    "\n",
    "# Load data from CSV file\n",
    "# csv_file = 'data.csv'  # Provide the path to your CSV file here\n",
    "# data, labels = load_data_from_csv(csv_file)\n",
    "\n",
    "print(\"Information Gain and Mini Impurity for each attribute:\")\n",
    "print_information_gain(X, Y)\n",
    "\n",
    "# Fit the decision tree\n",
    "tree = fit(X, Y)\n",
    "\n",
    "# Predictions for new samples\n",
    "new_samples = np.array([\n",
    "    [1, 0, 1, 1, 0],  # Example new sample\n",
    "    [0, 1, 0, 0, 1]   # Example new sample\n",
    "])\n",
    "for sample in new_samples:\n",
    "    prediction = predict(tree, sample)\n",
    "    print(f\"Prediction for {sample}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8788bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[2, 3] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m X \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     94\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 96\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m left_child, right_child \u001b[38;5;241m=\u001b[39m split(X, y, X\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(best_attribute), best_value)\n\u001b[1;32m     65\u001b[0m tree \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m'\u001b[39m: best_attribute,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: best_value,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m: fit(X\u001b[38;5;241m.\u001b[39miloc[left_child], y[left_child]),\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_child\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_child\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m }\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     best_attribute, best_value \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_attribute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mfind_best_split\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m attribute_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X[attribute])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m attribute_values:\n\u001b[0;32m---> 45\u001b[0m     left_child, right_child \u001b[38;5;241m=\u001b[39m \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     ig \u001b[38;5;241m=\u001b[39m information_gain(y, left_child, right_child)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ig \u001b[38;5;241m>\u001b[39m max_info_gain:\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(X, y, split_attribute, split_value)\u001b[0m\n\u001b[1;32m     28\u001b[0m left_child_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X\u001b[38;5;241m.\u001b[39miloc[:, split_attribute] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m split_value)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m right_child_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X\u001b[38;5;241m.\u001b[39miloc[:, split_attribute] \u001b[38;5;241m>\u001b[39m split_value)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m left_child \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_child_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m right_child \u001b[38;5;241m=\u001b[39m y[right_child_indices]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m left_child, right_child\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1033\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1068\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;66;03m# We need to decide whether to treat this as a positional indexer\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m-> 1068\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[2, 3] not in index'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Function to calculate entropy\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(y)\n",
    "    ps = class_counts / len(y)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "    return entropy\n",
    "\n",
    "def information_gain(parent, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Function to calculate information gain\n",
    "    \"\"\"\n",
    "    num_left = len(left_child) / len(parent)\n",
    "    num_right = len(right_child) / len(parent)\n",
    "    ig = entropy(parent) - (num_left * entropy(left_child) + num_right * entropy(right_child))\n",
    "    return ig\n",
    "\n",
    "def split(X, y, split_attribute, split_value):\n",
    "    \"\"\"\n",
    "    Function to split the dataset\n",
    "    \"\"\"\n",
    "    left_child_indices = np.where(X.iloc[:, split_attribute] <= split_value)[0]\n",
    "    right_child_indices = np.where(X.iloc[:, split_attribute] > split_value)[0]\n",
    "    left_child = y[left_child_indices]\n",
    "    right_child = y[right_child_indices]\n",
    "    return left_child, right_child\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    \"\"\"\n",
    "    Function to find the best attribute to split on\n",
    "    \"\"\"\n",
    "    best_attribute = None\n",
    "    best_value = None\n",
    "    max_info_gain = -1\n",
    "\n",
    "    for attribute in X.columns:\n",
    "        attribute_values = np.unique(X[attribute])\n",
    "        for value in attribute_values:\n",
    "            left_child, right_child = split(X, y, X.columns.get_loc(attribute), value)\n",
    "            ig = information_gain(y, left_child, right_child)\n",
    "            if ig > max_info_gain:\n",
    "                max_info_gain = ig\n",
    "                best_attribute = attribute\n",
    "                best_value = value\n",
    "\n",
    "    return best_attribute, best_value\n",
    "\n",
    "def fit(X, y):\n",
    "    \"\"\"\n",
    "    Function to fit the decision tree\n",
    "    \"\"\"\n",
    "    if len(set(y)) == 1:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    else:\n",
    "        best_attribute, best_value = find_best_split(X, y)\n",
    "        if best_attribute is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        left_child, right_child = split(X, y, X.columns.get_loc(best_attribute), best_value)\n",
    "        tree = {\n",
    "            'attribute': best_attribute,\n",
    "            'value': best_value,\n",
    "            'left': fit(X.iloc[left_child], y[left_child]),\n",
    "            'right': fit(X.iloc[right_child], y[right_child])\n",
    "        }\n",
    "        return tree\n",
    "\n",
    "def predict(tree, X):\n",
    "    \"\"\"\n",
    "    Function to predict using the decision tree\n",
    "    \"\"\"\n",
    "    if isinstance(tree, dict):\n",
    "        if X[tree['attribute']] <= tree['value']:\n",
    "            return predict(tree['left'], X)\n",
    "        else:\n",
    "            return predict(tree['right'], X)\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "# Example usage:\n",
    "data = {\n",
    "    'feature1': [2, 5, 8, 1, 4],\n",
    "    'feature2': [3, 1, 9, 8, 5],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']\n",
    "\n",
    "tree = fit(X, y)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1e1a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m X \u001b[38;5;241m=\u001b[39m df[feature_cols]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Extract features as numpy array\u001b[39;00m\n\u001b[1;32m     98\u001b[0m Y \u001b[38;5;241m=\u001b[39m df[target_col]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Extract target variable as numpy array\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     best_attribute, best_value \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_attribute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Counter(y)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mfind_best_split\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m attribute_values:\n\u001b[1;32m     45\u001b[0m     left_child, right_child \u001b[38;5;241m=\u001b[39m split(X, y, attribute, value)\n\u001b[0;32m---> 46\u001b[0m     ig \u001b[38;5;241m=\u001b[39m \u001b[43minformation_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_child\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ig \u001b[38;5;241m>\u001b[39m max_info_gain:\n\u001b[1;32m     48\u001b[0m         max_info_gain \u001b[38;5;241m=\u001b[39m ig\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36minformation_gain\u001b[0;34m(parent, left_child, right_child)\u001b[0m\n\u001b[1;32m     19\u001b[0m num_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(left_child) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(parent)\n\u001b[1;32m     20\u001b[0m num_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(right_child) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(parent)\n\u001b[0;32m---> 21\u001b[0m ig \u001b[38;5;241m=\u001b[39m \u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (num_left \u001b[38;5;241m*\u001b[39m entropy(left_child) \u001b[38;5;241m+\u001b[39m num_right \u001b[38;5;241m*\u001b[39m entropy(right_child))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ig\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mentropy\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentropy\u001b[39m(y):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Function to calculate entropy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     class_counts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     ps \u001b[38;5;241m=\u001b[39m class_counts \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     12\u001b[0m     entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum([p \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m ps \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Function to calculate entropy\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(y)\n",
    "    ps = class_counts / len(y)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "    return entropy\n",
    "\n",
    "def information_gain(parent, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Function to calculate information gain\n",
    "    \"\"\"\n",
    "    num_left = len(left_child) / len(parent)\n",
    "    num_right = len(right_child) / len(parent)\n",
    "    ig = entropy(parent) - (num_left * entropy(left_child) + num_right * entropy(right_child))\n",
    "    return ig\n",
    "\n",
    "def split(X, y, split_attribute, split_value):\n",
    "    \"\"\"\n",
    "    Function to split the dataset\n",
    "    \"\"\"\n",
    "    left_child_indices = np.where(X[:, split_attribute] <= split_value)[0]\n",
    "    right_child_indices = np.where(X[:, split_attribute] > split_value)[0]\n",
    "    left_child = y[left_child_indices]\n",
    "    right_child = y[right_child_indices]\n",
    "    return left_child, right_child\n",
    "\n",
    "def find_best_split(X, y):\n",
    "    \"\"\"\n",
    "    Function to find the best attribute to split on\n",
    "    \"\"\"\n",
    "    best_attribute = None\n",
    "    best_value = None\n",
    "    max_info_gain = -1\n",
    "\n",
    "    for attribute in range(X.shape[1]):\n",
    "        attribute_values = np.unique(X[:, attribute])\n",
    "        for value in attribute_values:\n",
    "            left_child, right_child = split(X, y, attribute, value)\n",
    "            ig = information_gain(y, left_child, right_child)\n",
    "            if ig > max_info_gain:\n",
    "                max_info_gain = ig\n",
    "                best_attribute = attribute\n",
    "                best_value = value\n",
    "\n",
    "    return best_attribute, best_value\n",
    "\n",
    "def fit(X, y):\n",
    "    \"\"\"\n",
    "    Function to fit the decision tree\n",
    "    \"\"\"\n",
    "    if len(set(y)) == 1:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    else:\n",
    "        best_attribute, best_value = find_best_split(X, y)\n",
    "        if best_attribute is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        left_child, right_child = split(X, y, best_attribute, best_value)\n",
    "        tree = {\n",
    "            'attribute': best_attribute,\n",
    "            'value': best_value,\n",
    "            'left': fit(X[left_child], y[left_child]),\n",
    "            'right': fit(X[right_child], y[right_child])\n",
    "        }\n",
    "        return tree\n",
    "\n",
    "def predict(tree, X):\n",
    "    \"\"\"\n",
    "    Function to predict using the decision tree\n",
    "    \"\"\"\n",
    "    if isinstance(tree, dict):\n",
    "        if X[tree['attribute']] <= tree['value']:\n",
    "            return predict(tree['left'], X)\n",
    "        else:\n",
    "            return predict(tree['right'], X)\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "# Example usage:\n",
    "data = {\n",
    "    'feature1': [2, 5, 8, 1, 4],\n",
    "    'feature2': [3, 1, 9, 8, 5],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "}\n",
    "df = pd.read_csv('/Users/mukulhooda/Documents/Workbooks /Assignment 2.csv')\n",
    "\n",
    "feature_cols = ['Day','Outlook','Humidity','Wind','Play Tennis']  # List of column names for features (X)\n",
    "target_col = 'Play Tennis'  # Name of the column for the target variable (Y)\n",
    "\n",
    "# Step 4: Extract X and Y\n",
    "X = df[feature_cols].values  # Extract features as numpy array\n",
    "Y = df[target_col].values  # Extract target variable as numpy array\n",
    "\n",
    "\n",
    "tree = fit(X, Y)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889e2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
